{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axandrei1234/Handwritten-Text-Recognition/blob/main/HandwrittenRecog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "24SLBtzXvcqh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import seaborn as sns\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX5zcMhmhm22"
      },
      "outputs": [],
      "source": [
        "def extractAndCenterROI(binaryImage, padding=10, maxSize=(32, 32)):\n",
        "    contours, _ = cv2.findContours(binaryImage, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if not contours:\n",
        "        print(\"No contours found.\")\n",
        "        return None\n",
        "\n",
        "    maxContour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    x, y, w, h = cv2.boundingRect(maxContour)\n",
        "\n",
        "    roi = binaryImage[y:y + h, x:x + w]\n",
        "    roiWithPadding = np.zeros((h + 2 * padding, w + 2 * padding), dtype=np.uint8)\n",
        "    roiWithPadding[padding:padding + h, padding:padding + w] = roi\n",
        "\n",
        "    roiResized = cv2.resize(roiWithPadding, maxSize, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    return roiResized\n",
        "\n",
        "\n",
        "def whiteMask(image, squareSize):\n",
        "    mask = np.zeros_like(image)\n",
        "    mask[:squareSize//2, :] = 255  # Top region\n",
        "    mask[-squareSize//2:, :] = 255  # Bottom region\n",
        "    mask[:, :squareSize//2] = 255  # Left region\n",
        "    mask[:, -squareSize//2:] = 255  # Right region\n",
        "    resultImage = np.where(mask == 255, 255, image)\n",
        "    return resultImage\n",
        "\n",
        "def increaseBrightness(img, value=30):\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    h, s, v = cv2.split(hsv)\n",
        "\n",
        "    lim = 255 - value\n",
        "    v[v > lim] = 255\n",
        "    v[v <= lim] += value\n",
        "\n",
        "    final_hsv = cv2.merge((h, s, v))\n",
        "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
        "    return img\n",
        "\n",
        "def adjustMeanValue(image, target_mean):\n",
        "    currentMean = np.mean(image)\n",
        "    adjustmentFactor = target_mean / currentMean\n",
        "    adjustedImage = np.clip(image * adjustmentFactor, 0, 255).astype(np.uint8)\n",
        "    return adjustedImage\n",
        "\n",
        "def adjustThickness(binaryImage, desiredThickness):\n",
        "    desiredThickness = max(0, desiredThickness)\n",
        "\n",
        "    kernelDilate = np.ones((desiredThickness, desiredThickness), np.uint8)\n",
        "    dilatedImage = cv2.dilate(binaryImage, kernelDilate, iterations=1)\n",
        "\n",
        "    kernelErode = np.ones((desiredThickness, desiredThickness), np.uint8)\n",
        "    erodedImage = cv2.erode(dilatedImage, kernelErode, iterations=1)\n",
        "\n",
        "    return erodedImage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cScWor0TEz7s"
      },
      "outputs": [],
      "source": [
        "def processImages(input_dir, output_dir, desired_thickness):\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    count = 1\n",
        "\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "            print(f\"Processing image: {filename}\")\n",
        "\n",
        "            input_path = os.path.join(input_dir, filename)\n",
        "            initialImage = cv2.resize(cv2.imread(input_path), (32,32), interpolation = cv2.INTER_CUBIC)\n",
        "            image = adjustMeanValue(increaseBrightness(cv2.imread(input_path), 20), 150)\n",
        "            imageGray = cv2.cvtColor(cv2.imread(input_path), cv2.COLOR_BGR2GRAY)\n",
        "            imageResized = cv2.resize(imageGray, (128, 128), interpolation=cv2.INTER_AREA)\n",
        "            thresh = 127\n",
        "            imageBinary = cv2.threshold(imageResized, thresh, 255, cv2.THRESH_BINARY)[1]\n",
        "            maskedImage = whiteMask(imageBinary, 64)\n",
        "            cv2.imwrite('temp.png', maskedImage)\n",
        "            imageBinaryResizedRead = cv2.imread('temp.png', cv2.IMREAD_GRAYSCALE)  # Read the saved image in grayscale\n",
        "\n",
        "            height, width = imageBinaryResizedRead.shape\n",
        "\n",
        "            for y in range(height):\n",
        "                for x in range(width):\n",
        "                    if imageBinaryResizedRead[y, x] == 255:\n",
        "                        imageBinaryResizedRead[y, x] = 0\n",
        "                    else:\n",
        "                        imageBinaryResizedRead[y, x] = 255\n",
        "            os.remove('temp.png')\n",
        "\n",
        "            newImage = cv2.GaussianBlur(imageBinaryResizedRead, (3, 3), 0.3)\n",
        "            roiImage = extractAndCenterROI(newImage, padding=2)\n",
        "\n",
        "            adjustedImage = adjustThickness(roiImage, desired_thickness)\n",
        "\n",
        "            finalImage = cv2.resize(adjustedImage, (32, 32), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            output_path = os.path.join(output_dir, f\"testImage{count}.png\")\n",
        "            cv2.imwrite(output_path, finalImage)\n",
        "            print(f\"Processed image saved to: {output_path}\")\n",
        "\n",
        "            count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgXAF5cb_STM"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    inputDirectory = 'inputPictures'\n",
        "    outputDirectory = 'processedImages'\n",
        "\n",
        "    desiredThickness = 0\n",
        "\n",
        "    processImages(inputDirectory, outputDirectory, desiredThickness)\n",
        "\n",
        "    shutil.make_archive('/content/processedImages', 'zip', '/content/processedImages')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij-okhPSExwM"
      },
      "outputs": [],
      "source": [
        "%rm -rf inputPictures\n",
        "%rm -rf processedImages\n",
        "%rm -rf dataset\n",
        "%rm -rf __MACOSX\n",
        "%rm -rf processedImages.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB2xa38lJIWG"
      },
      "outputs": [],
      "source": [
        "!unzip dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnNlH3qVpQv8"
      },
      "outputs": [],
      "source": [
        "directory = 'dataset/'\n",
        "df = pd.read_csv(directory + 'trainCVTest.csv')\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=31)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=31)\n",
        "\n",
        "# Extract file paths and labels for each set\n",
        "train_file_paths = train_df['file_name'].values\n",
        "train_labels = train_df['label'].values\n",
        "\n",
        "val_file_paths = val_df['file_name'].values\n",
        "val_labels = val_df['label'].values\n",
        "\n",
        "test_file_paths = test_df['file_name'].values\n",
        "test_labels = test_df['label'].values\n",
        "\n",
        "# Create a mapping from string labels to integer labels\n",
        "label_mapping = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4,\n",
        "                 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9,\n",
        "                 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14,\n",
        "                 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19,\n",
        "                 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24,\n",
        "                 'z': 25}\n",
        "label_mapping_inv = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "\n",
        "# Convert string labels to integer labels\n",
        "train_labels = [label_mapping[label] for label in train_labels]\n",
        "val_labels = [label_mapping[label] for label in val_labels]\n",
        "test_labels = [label_mapping[label] for label in test_labels]\n",
        "\n",
        "# Create separate datasets for train, validation, and test\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((train_file_paths, train_labels))\n",
        "ds_val = tf.data.Dataset.from_tensor_slices((val_file_paths, val_labels))\n",
        "ds_test = tf.data.Dataset.from_tensor_slices((test_file_paths, test_labels))\n",
        "\n",
        "def readImage(image_file, label):\n",
        "    image = tf.io.read_file(directory + image_file)\n",
        "    image = tf.image.decode_image(image, channels=1, dtype=tf.float32)\n",
        "    return image, label\n",
        "\n",
        "batchSize = 32\n",
        "ds_train = ds_train.map(readImage).batch(batchSize)\n",
        "ds_val = ds_val.map(readImage).batch(batchSize)\n",
        "ds_test = ds_test.map(readImage).batch(batchSize)\n",
        "\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu',input_shape=(32, 32, 1),  padding='same', name=\"Conv1\"))\n",
        "model.add(layers.MaxPooling2D((2, 2), name='MaxPooling1'))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='Conv2'))\n",
        "model.add(layers.MaxPooling2D((2, 2), name='MaxPooling2'))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', name=\"Conv3\"))\n",
        "model.add(layers.MaxPooling2D((2, 2), name='MaxPooling3'))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', name=\"Conv4\"))\n",
        "model.add(layers.MaxPooling2D((2, 2), name='MaxPooling4'))\n",
        "model.add(layers.Flatten(name='Flatten'))\n",
        "model.add(layers.Dense(128, activation='relu', name='Dense1'))\n",
        "model.add(layers.Dropout(0.55))\n",
        "model.add(layers.Dense(64, activation='relu', name='Dense2'))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(26, activation='softmax', name='FinalLayer'))\n",
        "\n",
        "# model = models.Sequential()\n",
        "# model.add(layers.Conv2D(128, (3, 3), activation='relu', input_shape=(32, 32, 1), padding='same', name=\"Conv1\"))\n",
        "# model.add(layers.MaxPooling2D((2, 2), name='MaxPooling1'))\n",
        "# model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='Conv2'))\n",
        "# model.add(layers.MaxPooling2D((2, 2), name='MaxPooling2'))\n",
        "# model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same', name=\"Conv3\"))\n",
        "# model.add(layers.MaxPooling2D((2, 2), name='MaxPooling3'))\n",
        "# model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', name=\"Conv4\"))\n",
        "# model.add(layers.MaxPooling2D((2, 2), name='MaxPooling4'))\n",
        "# model.add(layers.Flatten(name='Flatten'))\n",
        "# model.add(layers.Dense(256, activation='relu', name='Dense1'))\n",
        "# model.add(layers.Dropout(0.5))\n",
        "# model.add(layers.Dense(128, activation='relu', name='Dense2'))\n",
        "# model.add(layers.Dropout(0.3))\n",
        "# model.add(layers.Dense(26, activation='softmax', name='FinalLayer'))\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer= tf.keras.optimizers.Adam(0.0013),\n",
        "              loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "numEpochs = 15\n",
        "\n",
        "plot_model(model, to_file='my_CNN_plot.png', show_shapes=True, show_layer_names=True)\n",
        "history = model.fit(ds_train, epochs = numEpochs, validation_data=ds_val)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD7tZ_IGS4j4"
      },
      "outputs": [],
      "source": [
        "training_accuracy = history.history['accuracy']\n",
        "validation_accuracy = history.history['val_accuracy']\n",
        "\n",
        "training_loss = history.history['loss']\n",
        "validation_loss = history.history['loss']\n",
        "\n",
        "epochs = range(1, numEpochs + 1)  # Replace num_epochs with the actual number of training epochs\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=False, figsize=(6, 6))\n",
        "\n",
        "# Plotting the training and cross-validation accuracy\n",
        "ax1.plot(epochs, training_accuracy, label='Training Accuracy')\n",
        "ax1.plot(epochs, validation_accuracy, label='Cross-Validation Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_title('Training and Cross-Validation Accuracy Over Epochs')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(epochs, training_loss, label='Training Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('Training and Cross-Validation Loss over Epochs')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uW4xkO9NI5mT"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(ds_test)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "\n",
        "true_labels = np.concatenate([label.numpy() for _, label in ds_test], axis=0)\n",
        "\n",
        "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
        "predicted_labels_str = [reverse_label_mapping[label] for label in predicted_labels]\n",
        "true_labels_str = [reverse_label_mapping[label] for label in true_labels]\n",
        "\n",
        "conf_matrix = confusion_matrix(true_labels_str, predicted_labels_str)\n",
        "\n",
        "print(\"Predicting the test set\")\n",
        "for true_label, predicted_label in zip(true_labels_str, predicted_labels_str):\n",
        "    print(f'True Label: {true_label}, Predicted Label: {predicted_label}')\n",
        "\n",
        "accuracy = accuracy_score(true_labels_str, predicted_labels_str)\n",
        "\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcwEpgCSpqJq"
      },
      "outputs": [],
      "source": [
        "misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
        "\n",
        "misclassified_images = []\n",
        "misclassified_true_labels = []\n",
        "misclassified_predicted_labels = []\n",
        "\n",
        "for i in misclassified_indices:\n",
        "    # Get the ith image and label from the dataset\n",
        "    image, true_label = list(ds_test.unbatch().as_numpy_iterator())[i]\n",
        "    misclassified_images.append(image)\n",
        "    misclassified_true_labels.append(true_labels_str[i])\n",
        "    misclassified_predicted_labels.append(predicted_labels_str[i])\n",
        "\n",
        "# Display misclassified images with true and predicted labels\n",
        "num_misclassified = len(misclassified_images)\n",
        "if num_misclassified > 0:\n",
        "    # Set the number of images per row\n",
        "    images_per_row = 3\n",
        "    num_rows = int(np.ceil(num_misclassified / images_per_row))\n",
        "\n",
        "    plt.figure(figsize=(15, 5 * num_rows))\n",
        "    for i in range(num_misclassified):\n",
        "        plt.subplot(num_rows, images_per_row, i + 1)\n",
        "        plt.imshow(np.squeeze(misclassified_images[i]), cmap='gray')\n",
        "        plt.title(f'True: {misclassified_true_labels[i]}\\nPredicted: {misclassified_predicted_labels[i]}')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No misclassified images to display.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZdpMurquT-0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FDHtEReqZ3w"
      },
      "outputs": [],
      "source": [
        "!unzip testSetProcessed.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHHkbpuNoKe9"
      },
      "outputs": [],
      "source": [
        "#new prediction\n",
        "directory = 'testSetProcessed/'\n",
        "prediction_label= []\n",
        "images = []\n",
        "\n",
        "# Sort filenames using a custom sorting key\n",
        "image_filenames = sorted([f for f in os.listdir(directory) if f.endswith(\".png\")], key=lambda x: int(x.split('testImage')[1].split('.png')[0]))\n",
        "\n",
        "for imageFilename in image_filenames:\n",
        "    img_path = os.path.join(directory, imageFilename)\n",
        "\n",
        "    # Read and decode the image using TensorFlow functions\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_image(img, channels=1, dtype=tf.float32)\n",
        "    img_array = np.expand_dims(img, axis=0)\n",
        "    images.append(img_array)\n",
        "\n",
        "images = np.vstack(images)\n",
        "predictions = model.predict(images)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "predicted_labels_str = [reverse_label_mapping[label] for label in predicted_labels]\n",
        "\n",
        "for filename, prediction in zip(image_filenames, predicted_labels_str):\n",
        "    print(f\"File: {filename}, Prediction: {prediction}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqiuCtISUOlZndodFEdbXk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}